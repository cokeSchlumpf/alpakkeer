{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Alpakkeer Build, Run and Operate Akka Streams applications quickly, ready for production. Documentation: cokeschlumpf.github.io/alpakkeer GitHub Repository: github.com/cokeSchlumpf/alpakkeer Alpakkeer is an opinionated Java+Scala toolkit to build, run and operate light-weight integration applications based on Akka Streams and Alpakka . Alpakkeer bundles various libraries and components: A Web Server based on Javalin to provide simple access via REST APIs to manage Akka Streams processes and expose metrics to Prometheus and Grafana. Configuration Management based on Lightbend Config including some extensions for environment-based configurations and automatic mapping to POJOs. Prometheus Client to record application and stream metrics. Alpakkeer also provides custom FlowStages to argument your stream with Akka Streams specific metrics. Abstracted building blocks to compose streams based on Alpakka . An easy to use DSL for Java and Scala to compose applications. Getting Started Add the dependency: Maven <dependency> <groupId> io.github.cokeschlumpf </groupId> <artifactId> alpakkeer-core </artifactId> <version> 0.0.1-SNAPSHOT </version> </dependency> Gradle compile group: 'io.github.cokeschlumpf' , name: 'alpakkeer-core' , version: '0.0.1-SNAPSHOT' SBT libraryDependencies += \"io.github.cokeschlumpf\" %% \"alpakkeer-scaladsl\" % \"0.0.1-SNAPSHOT\" To use the latest snapshot version, add the snapshot repository to your configuration: Maven <repositories> <repository> <id> sonatype-oss </id> <name> Sonatype OSS Repository </name> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </repository> </repositories> Gradle repositories { jcenter () maven { url \"https://oss.sonatype.org/content/repositories/snapshots\" } } SBT resolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\" Start coding Java import akka.stream.javadsl.Keep ; import akka.stream.javadsl.Sink ; import akka.stream.javadsl.Source ; import alpakkeer.core.scheduler.model.CronExpression ; import alpakkeer.javadsl.Alpakkeer ; public class HelloAlpakkeer { public static void main ( String ... args ) { Alpakkeer . create () . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) . withScheduledExecution ( CronExpression . everySeconds ( 10 )) . withLoggingMonitor ()) . start (); } } Scala import akka.stream.scaladsl. { Keep , Sink , Source } import alpakkeer.core.scheduler.model.CronExpression import alpakkeer.javadsl.Alpakkeer object HelloAlpakkeer extends App { Alpakkeer . create () . withJob ( b => b . create ( \"hello-world\" ) . runGraph ( Source . single ( \"Hello World!\" ) . toMat ( Sink . foreach ( println ))( Keep . right ) . asJava ) . withLoggingMonitor () . withScheduledExecution ( CronExpression . everySeconds ( 10 ))) . start () } Requirements Java 11+, Scala 2.13 Alpakkeer runs on: Akka 2.6.4+, Akka Streams 2.6.4+, Javalin 3.8.0 License This project is licensed under the terms of the Apache 2.0 License.","title":"Getting Started"},{"location":"#alpakkeer","text":"Build, Run and Operate Akka Streams applications quickly, ready for production. Documentation: cokeschlumpf.github.io/alpakkeer GitHub Repository: github.com/cokeSchlumpf/alpakkeer Alpakkeer is an opinionated Java+Scala toolkit to build, run and operate light-weight integration applications based on Akka Streams and Alpakka . Alpakkeer bundles various libraries and components: A Web Server based on Javalin to provide simple access via REST APIs to manage Akka Streams processes and expose metrics to Prometheus and Grafana. Configuration Management based on Lightbend Config including some extensions for environment-based configurations and automatic mapping to POJOs. Prometheus Client to record application and stream metrics. Alpakkeer also provides custom FlowStages to argument your stream with Akka Streams specific metrics. Abstracted building blocks to compose streams based on Alpakka . An easy to use DSL for Java and Scala to compose applications.","title":"Alpakkeer"},{"location":"#getting-started","text":"Add the dependency: Maven <dependency> <groupId> io.github.cokeschlumpf </groupId> <artifactId> alpakkeer-core </artifactId> <version> 0.0.1-SNAPSHOT </version> </dependency> Gradle compile group: 'io.github.cokeschlumpf' , name: 'alpakkeer-core' , version: '0.0.1-SNAPSHOT' SBT libraryDependencies += \"io.github.cokeschlumpf\" %% \"alpakkeer-scaladsl\" % \"0.0.1-SNAPSHOT\" To use the latest snapshot version, add the snapshot repository to your configuration: Maven <repositories> <repository> <id> sonatype-oss </id> <name> Sonatype OSS Repository </name> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </repository> </repositories> Gradle repositories { jcenter () maven { url \"https://oss.sonatype.org/content/repositories/snapshots\" } } SBT resolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\" Start coding Java import akka.stream.javadsl.Keep ; import akka.stream.javadsl.Sink ; import akka.stream.javadsl.Source ; import alpakkeer.core.scheduler.model.CronExpression ; import alpakkeer.javadsl.Alpakkeer ; public class HelloAlpakkeer { public static void main ( String ... args ) { Alpakkeer . create () . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) . withScheduledExecution ( CronExpression . everySeconds ( 10 )) . withLoggingMonitor ()) . start (); } } Scala import akka.stream.scaladsl. { Keep , Sink , Source } import alpakkeer.core.scheduler.model.CronExpression import alpakkeer.javadsl.Alpakkeer object HelloAlpakkeer extends App { Alpakkeer . create () . withJob ( b => b . create ( \"hello-world\" ) . runGraph ( Source . single ( \"Hello World!\" ) . toMat ( Sink . foreach ( println ))( Keep . right ) . asJava ) . withLoggingMonitor () . withScheduledExecution ( CronExpression . everySeconds ( 10 ))) . start () }","title":"Getting Started"},{"location":"#requirements","text":"Java 11+, Scala 2.13 Alpakkeer runs on: Akka 2.6.4+, Akka Streams 2.6.4+, Javalin 3.8.0","title":"Requirements"},{"location":"#license","text":"This project is licensed under the terms of the Apache 2.0 License.","title":"License"},{"location":"concepts/","text":"Concepts Single Project Ports and Adapters Convention over Configuration Extensibility","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#single-project","text":"","title":"Single Project"},{"location":"concepts/#ports-and-adapters","text":"","title":"Ports and Adapters"},{"location":"concepts/#convention-over-configuration","text":"","title":"Convention over Configuration"},{"location":"concepts/#extensibility","text":"","title":"Extensibility"},{"location":"configuration/","text":"","title":"Configuration"},{"location":"deployment/","text":"Deployment def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> _ int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Deployment"},{"location":"deployment/#deployment","text":"def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> _ int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Deployment"},{"location":"jobs/","text":"Jobs Alpakkeer jobs are wrappers around streams with sources which have a definite count of records to process (e.g. JDBC sources, directory listings, etc.)). Jobs can be triggered following a schedule, manually via API call or programmatically via its API. Alpakkeer ensures that different executions of a job are not running in parallel. If a job is already running when it is triggered, it can be either queued or discarded. When triggering jobs, properties can be passed to it to customize its behaviour; jobs also have a context to share state between executions of a job. Defining Jobs A job can be defined using a builder API. Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO A job can be triggered using Properties and Context Every job has Properties type and a Context type. By default these types are set to alpakkeer.core.values.Nothing and akka.Done . Job poperties are the arguments which are passed to a job when it is triggered. The properties can be used while constructing the stream. Every job has default properties which are used if no properties are explicitly passed while triggering. The job's context can be seen as the shared state between job executions. The current context of the job is passed to the stream constructor, when the job is triggered - just like the properties. The result aka the materialzed value of the job execution is the updated context which will be passed to the next execution. To make the context persistent between restarts of the application, Alpakkeer by default offers file-based and database based context persistence. The job types can be defined by passing default properties and the initial context to the job builder API: Java @Value @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Properties { final String name ; } @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Context { Context addGreeted ( String name ) { // ... } } alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" , Properties . apply ( \"Anonymous\" ), Context . apply ()) . runGraph ( b -> { var name = b . getProperties (). getName (); var hello = \"Hello \" + name + \"!\" ; var nextContext = b . getContext (). addGreeted ( name ); return Source . single () . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()) . mapMaterializedValue ( done -> done . thenApply ( i -> nextContext )); }) To enable serialization for the REST API as well as for Alpakkeer's built-in persistence providers, make sure that the class is serializable and de-serializable with Jackson. Triggering Jobs To trigger a job via the REST API with its default properties, call: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/jobs/hello-world To trigger the job with properties, just pass them within the request body: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }' \\ http://localhost:8042/api/v1/jobs/hello-world Jobs can also be triggered programmatically: var app = Alpakkeer . create () /* ... job definitions ... */ . start (); var job = app . getResources (). < Context , Properties > getJob ( \"hello-world\" ); job . start (); job . start ( false ); // do not queue job . start ( false , Properties . apply ( \"Emil\" )); Scheduling Jobs Jobs can be configured to be triggered following a cron expression based schedule. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withScheduledExecution ( CronExpression . apply ( \"0 0 12 * * ?\" ), Properties . apply ( \"Emil\" ), false )) Scala CronExpression offer various helper methods to simplify the creation of the expression, e.g. CronExpression.everySeconds(10) . You may register multiple expressions to schedule different kinds of executions for the job, e.g. an incremental-load every day and a full-load every month for crawler applications. Logging & Monitoring Jobs can be monitored using the interface alpakkeer.core.jobs.monitor.JobMonitor . The monitor can implement methods to collect information about stream execution time, exceptions, results and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface JobMonitor < P , C > { void onTriggered ( String executionId , P properties ); void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause ); void onCompleted ( String executionId , C result ); void onCompleted ( String executionId ); void onStopped ( String executionId , C result ); void onStopped ( String executionId ); void onQueued ( int newQueueSize ); void onEnqueued ( int newQueueSize ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the job status in the job's REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala Aa job can be configured to have multipe monitors of course. Persisting Context By default all job contexts are kept in-memory only. The context store can be defined in the application configuration: application.conf alpakkeer.contexts.type = \"in-memory\" Other possible values are fs (file-system) or db (database). The context store can also be configured programmatically: Java Alpakkeer . create () . configure ( r -> r . withContextStore ( ContextStores . fileSystem ())) . withJob ( /* ... */ ) /* ... */ . start () Scala Disabling Jobs Jobs can be disabled programmatically. This feature can be used to disable jobs by application configurations. See also Deployment for more information. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isJobEnabled ())) Scala Custom API endpoints The default REST API can be extended with custom REST endpoints, e.g. to trigger the job with custom inputs than job properties. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , job ) -> { api . get ( \"process\" , ctx -> { ctx . result ( job . start (). toCompletableFuture ()); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information. Configuration Overrides Job configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. schedule) in various environments. The default configuration key is alpakkeer.jobs : application.conf alpakkeer.jobs = [ { name = \"hello-world\" enabled = true clear-monitors = false clear-schedule = true monitors = [\"logging\"] schedule = [ { cron-expression = \"0 0 12 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }\"\"\" }, { cron-expression = \"0 0 6 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Antigone\"} }\"\"\" } ] } ] In the job definition, these configurations can be used: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala REST API Jobs can be managed using Alpakkeers REST API. The endpoints for jobs are: GET /api/v1/jobs - List all configured jobs GET /api/v1/jobs/{name} - Get job details POST /api/v1/jobs/{name} - Trigger job execution DELETE /api/v1/jobs/{name} - Stop current job execution GET /api/v1/jobs/{name}/sample - Get example properties which need to be posted when triggering the job For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"Jobs"},{"location":"jobs/#jobs","text":"Alpakkeer jobs are wrappers around streams with sources which have a definite count of records to process (e.g. JDBC sources, directory listings, etc.)). Jobs can be triggered following a schedule, manually via API call or programmatically via its API. Alpakkeer ensures that different executions of a job are not running in parallel. If a job is already running when it is triggered, it can be either queued or discarded. When triggering jobs, properties can be passed to it to customize its behaviour; jobs also have a context to share state between executions of a job.","title":"Jobs"},{"location":"jobs/#defining-jobs","text":"A job can be defined using a builder API. Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO A job can be triggered using","title":"Defining Jobs"},{"location":"jobs/#properties-and-context","text":"Every job has Properties type and a Context type. By default these types are set to alpakkeer.core.values.Nothing and akka.Done . Job poperties are the arguments which are passed to a job when it is triggered. The properties can be used while constructing the stream. Every job has default properties which are used if no properties are explicitly passed while triggering. The job's context can be seen as the shared state between job executions. The current context of the job is passed to the stream constructor, when the job is triggered - just like the properties. The result aka the materialzed value of the job execution is the updated context which will be passed to the next execution. To make the context persistent between restarts of the application, Alpakkeer by default offers file-based and database based context persistence. The job types can be defined by passing default properties and the initial context to the job builder API: Java @Value @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Properties { final String name ; } @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Context { Context addGreeted ( String name ) { // ... } } alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" , Properties . apply ( \"Anonymous\" ), Context . apply ()) . runGraph ( b -> { var name = b . getProperties (). getName (); var hello = \"Hello \" + name + \"!\" ; var nextContext = b . getContext (). addGreeted ( name ); return Source . single () . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()) . mapMaterializedValue ( done -> done . thenApply ( i -> nextContext )); }) To enable serialization for the REST API as well as for Alpakkeer's built-in persistence providers, make sure that the class is serializable and de-serializable with Jackson.","title":"Properties and Context"},{"location":"jobs/#triggering-jobs","text":"To trigger a job via the REST API with its default properties, call: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/jobs/hello-world To trigger the job with properties, just pass them within the request body: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }' \\ http://localhost:8042/api/v1/jobs/hello-world Jobs can also be triggered programmatically: var app = Alpakkeer . create () /* ... job definitions ... */ . start (); var job = app . getResources (). < Context , Properties > getJob ( \"hello-world\" ); job . start (); job . start ( false ); // do not queue job . start ( false , Properties . apply ( \"Emil\" ));","title":"Triggering Jobs"},{"location":"jobs/#scheduling-jobs","text":"Jobs can be configured to be triggered following a cron expression based schedule. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withScheduledExecution ( CronExpression . apply ( \"0 0 12 * * ?\" ), Properties . apply ( \"Emil\" ), false )) Scala CronExpression offer various helper methods to simplify the creation of the expression, e.g. CronExpression.everySeconds(10) . You may register multiple expressions to schedule different kinds of executions for the job, e.g. an incremental-load every day and a full-load every month for crawler applications.","title":"Scheduling Jobs"},{"location":"jobs/#logging-monitoring","text":"Jobs can be monitored using the interface alpakkeer.core.jobs.monitor.JobMonitor . The monitor can implement methods to collect information about stream execution time, exceptions, results and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface JobMonitor < P , C > { void onTriggered ( String executionId , P properties ); void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause ); void onCompleted ( String executionId , C result ); void onCompleted ( String executionId ); void onStopped ( String executionId , C result ); void onStopped ( String executionId ); void onQueued ( int newQueueSize ); void onEnqueued ( int newQueueSize ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the job status in the job's REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala Aa job can be configured to have multipe monitors of course.","title":"Logging &amp; Monitoring"},{"location":"jobs/#persisting-context","text":"By default all job contexts are kept in-memory only. The context store can be defined in the application configuration: application.conf alpakkeer.contexts.type = \"in-memory\" Other possible values are fs (file-system) or db (database). The context store can also be configured programmatically: Java Alpakkeer . create () . configure ( r -> r . withContextStore ( ContextStores . fileSystem ())) . withJob ( /* ... */ ) /* ... */ . start () Scala","title":"Persisting Context"},{"location":"jobs/#disabling-jobs","text":"Jobs can be disabled programmatically. This feature can be used to disable jobs by application configurations. See also Deployment for more information. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isJobEnabled ())) Scala","title":"Disabling Jobs"},{"location":"jobs/#custom-api-endpoints","text":"The default REST API can be extended with custom REST endpoints, e.g. to trigger the job with custom inputs than job properties. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , job ) -> { api . get ( \"process\" , ctx -> { ctx . result ( job . start (). toCompletableFuture ()); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information.","title":"Custom API endpoints"},{"location":"jobs/#configuration-overrides","text":"Job configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. schedule) in various environments. The default configuration key is alpakkeer.jobs : application.conf alpakkeer.jobs = [ { name = \"hello-world\" enabled = true clear-monitors = false clear-schedule = true monitors = [\"logging\"] schedule = [ { cron-expression = \"0 0 12 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }\"\"\" }, { cron-expression = \"0 0 6 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Antigone\"} }\"\"\" } ] } ] In the job definition, these configurations can be used: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala","title":"Configuration Overrides"},{"location":"jobs/#rest-api","text":"Jobs can be managed using Alpakkeers REST API. The endpoints for jobs are: GET /api/v1/jobs - List all configured jobs GET /api/v1/jobs/{name} - Get job details POST /api/v1/jobs/{name} - Trigger job execution DELETE /api/v1/jobs/{name} - Stop current job execution GET /api/v1/jobs/{name}/sample - Get example properties which need to be posted when triggering the job For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"REST API"},{"location":"monitoring/","text":"Monitoring & Metrics","title":"Monitoring & Metrics"},{"location":"monitoring/#monitoring-metrics","text":"","title":"Monitoring &amp; Metrics"},{"location":"processes/","text":"Processes Defining Processes Starting and Stopping Logging & Monitoring Disabling Processes API endpoints Configuration Overrides REST API","title":"Processes"},{"location":"processes/#processes","text":"","title":"Processes"},{"location":"processes/#defining-processes","text":"","title":"Defining Processes"},{"location":"processes/#starting-and-stopping","text":"","title":"Starting and Stopping"},{"location":"processes/#logging-monitoring","text":"","title":"Logging &amp; Monitoring"},{"location":"processes/#disabling-processes","text":"","title":"Disabling Processes"},{"location":"processes/#api-endpoints","text":"","title":"API endpoints"},{"location":"processes/#configuration-overrides","text":"","title":"Configuration Overrides"},{"location":"processes/#rest-api","text":"","title":"REST API"},{"location":"stream-utilities/","text":"Stream Utilities Alpakkeer offers easy accessible building-blogs for composing reactive streams with Akka Streams. Messaging Stream Monitors Error Handling","title":"Stream Utilities"},{"location":"stream-utilities/#stream-utilities","text":"Alpakkeer offers easy accessible building-blogs for composing reactive streams with Akka Streams.","title":"Stream Utilities"},{"location":"stream-utilities/#messaging","text":"","title":"Messaging"},{"location":"stream-utilities/#stream-monitors","text":"","title":"Stream Monitors"},{"location":"stream-utilities/#error-handling","text":"","title":"Error Handling"}]}