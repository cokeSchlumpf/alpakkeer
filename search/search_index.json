{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Alpakkeer Build, Run and Operate Akka Streams applications quickly, ready for production. Documentation: cokeschlumpf.github.io/alpakkeer GitHub Repository: github.com/cokeSchlumpf/alpakkeer Alpakkeer is an opinionated Java+Scala toolkit to build, run and operate light-weight integration applications based on Akka Streams and Alpakka . Alpakkeer bundles various libraries and components: A Web Server based on Javalin to provide simple access via REST APIs to manage Akka Streams processes and expose metrics to Prometheus and Grafana. Configuration Management based on Lightbend Config including some extensions for environment-based configurations and automatic mapping to POJOs. Prometheus Client to record application and stream metrics. Alpakkeer also provides custom FlowStages to argument your stream with Akka Streams specific metrics. Abstracted building blocks to compose streams based on Alpakka . An easy to use DSL for Java and Scala to compose applications. Getting Started Add the dependency: Maven <dependency> <groupId> io.github.cokeschlumpf </groupId> <artifactId> alpakkeer-core </artifactId> <version> 0.0.1-SNAPSHOT </version> </dependency> Gradle compile group: 'io.github.cokeschlumpf' , name: 'alpakkeer-core' , version: '0.0.1-SNAPSHOT' SBT libraryDependencies += \"io.github.cokeschlumpf\" %% \"alpakkeer-scaladsl\" % \"0.0.1-SNAPSHOT\" To use the latest snapshot version, add the snapshot repository to your configuration: Maven <repositories> <repository> <id> sonatype-oss </id> <name> Sonatype OSS Repository </name> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </repository> </repositories> Gradle repositories { jcenter () maven { url \"https://oss.sonatype.org/content/repositories/snapshots\" } } SBT resolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\" Start coding Java import akka.stream.javadsl.Keep ; import akka.stream.javadsl.Sink ; import akka.stream.javadsl.Source ; import alpakkeer.core.scheduler.model.CronExpression ; import alpakkeer.javadsl.Alpakkeer ; public class HelloAlpakkeer { public static void main ( String ... args ) { Alpakkeer . create () . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) . withScheduledExecution ( CronExpression . everySeconds ( 10 )) . withLoggingMonitor ()) . start (); } } Scala import akka.stream.scaladsl. { Keep , Sink , Source } import alpakkeer.core.scheduler.model.CronExpression import alpakkeer.javadsl.Alpakkeer object HelloAlpakkeer extends App { Alpakkeer . create () . withJob ( b => b . create ( \"hello-world\" ) . runGraph ( Source . single ( \"Hello World!\" ) . toMat ( Sink . foreach ( println ))( Keep . right ) . asJava ) . withLoggingMonitor () . withScheduledExecution ( CronExpression . everySeconds ( 10 ))) . start () } Requirements Java 11+, Scala 2.13 Alpakkeer runs on: Akka 2.6.4+, Akka Streams 2.6.4+, Javalin 3.8.0 License This project is licensed under the terms of the Apache 2.0 License.","title":"Getting Started"},{"location":"#alpakkeer","text":"Build, Run and Operate Akka Streams applications quickly, ready for production. Documentation: cokeschlumpf.github.io/alpakkeer GitHub Repository: github.com/cokeSchlumpf/alpakkeer Alpakkeer is an opinionated Java+Scala toolkit to build, run and operate light-weight integration applications based on Akka Streams and Alpakka . Alpakkeer bundles various libraries and components: A Web Server based on Javalin to provide simple access via REST APIs to manage Akka Streams processes and expose metrics to Prometheus and Grafana. Configuration Management based on Lightbend Config including some extensions for environment-based configurations and automatic mapping to POJOs. Prometheus Client to record application and stream metrics. Alpakkeer also provides custom FlowStages to argument your stream with Akka Streams specific metrics. Abstracted building blocks to compose streams based on Alpakka . An easy to use DSL for Java and Scala to compose applications.","title":"Alpakkeer"},{"location":"#getting-started","text":"Add the dependency: Maven <dependency> <groupId> io.github.cokeschlumpf </groupId> <artifactId> alpakkeer-core </artifactId> <version> 0.0.1-SNAPSHOT </version> </dependency> Gradle compile group: 'io.github.cokeschlumpf' , name: 'alpakkeer-core' , version: '0.0.1-SNAPSHOT' SBT libraryDependencies += \"io.github.cokeschlumpf\" %% \"alpakkeer-scaladsl\" % \"0.0.1-SNAPSHOT\" To use the latest snapshot version, add the snapshot repository to your configuration: Maven <repositories> <repository> <id> sonatype-oss </id> <name> Sonatype OSS Repository </name> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </repository> </repositories> Gradle repositories { jcenter () maven { url \"https://oss.sonatype.org/content/repositories/snapshots\" } } SBT resolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\" Start coding Java import akka.stream.javadsl.Keep ; import akka.stream.javadsl.Sink ; import akka.stream.javadsl.Source ; import alpakkeer.core.scheduler.model.CronExpression ; import alpakkeer.javadsl.Alpakkeer ; public class HelloAlpakkeer { public static void main ( String ... args ) { Alpakkeer . create () . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) . withScheduledExecution ( CronExpression . everySeconds ( 10 )) . withLoggingMonitor ()) . start (); } } Scala import akka.stream.scaladsl. { Keep , Sink , Source } import alpakkeer.core.scheduler.model.CronExpression import alpakkeer.javadsl.Alpakkeer object HelloAlpakkeer extends App { Alpakkeer . create () . withJob ( b => b . create ( \"hello-world\" ) . runGraph ( Source . single ( \"Hello World!\" ) . toMat ( Sink . foreach ( println ))( Keep . right ) . asJava ) . withLoggingMonitor () . withScheduledExecution ( CronExpression . everySeconds ( 10 ))) . start () }","title":"Getting Started"},{"location":"#requirements","text":"Java 11+, Scala 2.13 Alpakkeer runs on: Akka 2.6.4+, Akka Streams 2.6.4+, Javalin 3.8.0","title":"Requirements"},{"location":"#license","text":"This project is licensed under the terms of the Apache 2.0 License.","title":"License"},{"location":"concepts/","text":"Concepts Alpakkeer is a toolkit which bundles various other frameworks and tools to make it easy to develop streaming applications based on Akka Streams. Such applications are usually used for creating simple application integrations, prediction pipelines which integrate enterprise data with AI services, or automated machine learning pipelines. Ports and Adapters The design of Alpakkeer aims to let developers focus on actual business requirements and business logic of the application instead of technical details like integrating streaming technologies like Kafka or Flink, or storage technologies like object storage, document databases etc.. Thus Alpakkeer strictly follows the Ports and Adapters Pattern 1 and provides deault ports for messaging, job and stream management as well as default implementations for these ports with various technologies. With Alpakkeer applications can be run on minimal resources on a local machine, but also distributed on scalable cloud infrastructure and robust clusters for messaging and data stores, just by changing the application configuration. For all its default ports, Alpakkeer offers simple in-memory implementations (e.g. for messaging, context store) which can be used for local development, demos or very simple application which have no requirements for robustness. Single Project - Distributed Deployment Systems which are composed of multiple streams often share common data models or common stream logics. Also running the whole system to make manual tests and experiments for a optimal stream topology design help to quickly develop the application. Thus streams and services which might run distribited in a real system are easier to develop if all source code belongs to one project. For example, the following stream topology can be put in a single Alpakkeer Java Application. The crawlers can be implemented with Alpakkeer Jobs. The enricher and the processing streams can be implemented using Alpakkeer Processes. But at runtime, the application can run in different roles in multiple Kubernetes Deployments or on different virtual machines. See Configuration for more details. Convention over Configuration Alpakkeers DSL and configuration always offers fine-grained settings, but also all API methods have alternatives with minimal user-input. Also all configurations have senseful defaults. Thus most Alpakkeer applications don't require much customization on defaults if application designs follow common conventions. Extensibility All internal Alpakkeer components are defined by Interfaces. Thus almost all components can be replaced or extended with custom implementations. Ports & Adapters image source: Wikimedia \u21a9","title":"Concepts"},{"location":"concepts/#concepts","text":"Alpakkeer is a toolkit which bundles various other frameworks and tools to make it easy to develop streaming applications based on Akka Streams. Such applications are usually used for creating simple application integrations, prediction pipelines which integrate enterprise data with AI services, or automated machine learning pipelines.","title":"Concepts"},{"location":"concepts/#ports-and-adapters","text":"The design of Alpakkeer aims to let developers focus on actual business requirements and business logic of the application instead of technical details like integrating streaming technologies like Kafka or Flink, or storage technologies like object storage, document databases etc.. Thus Alpakkeer strictly follows the Ports and Adapters Pattern 1 and provides deault ports for messaging, job and stream management as well as default implementations for these ports with various technologies. With Alpakkeer applications can be run on minimal resources on a local machine, but also distributed on scalable cloud infrastructure and robust clusters for messaging and data stores, just by changing the application configuration. For all its default ports, Alpakkeer offers simple in-memory implementations (e.g. for messaging, context store) which can be used for local development, demos or very simple application which have no requirements for robustness.","title":"Ports and Adapters"},{"location":"concepts/#single-project-distributed-deployment","text":"Systems which are composed of multiple streams often share common data models or common stream logics. Also running the whole system to make manual tests and experiments for a optimal stream topology design help to quickly develop the application. Thus streams and services which might run distribited in a real system are easier to develop if all source code belongs to one project. For example, the following stream topology can be put in a single Alpakkeer Java Application. The crawlers can be implemented with Alpakkeer Jobs. The enricher and the processing streams can be implemented using Alpakkeer Processes. But at runtime, the application can run in different roles in multiple Kubernetes Deployments or on different virtual machines. See Configuration for more details.","title":"Single Project - Distributed Deployment"},{"location":"concepts/#convention-over-configuration","text":"Alpakkeers DSL and configuration always offers fine-grained settings, but also all API methods have alternatives with minimal user-input. Also all configurations have senseful defaults. Thus most Alpakkeer applications don't require much customization on defaults if application designs follow common conventions.","title":"Convention over Configuration"},{"location":"concepts/#extensibility","text":"All internal Alpakkeer components are defined by Interfaces. Thus almost all components can be replaced or extended with custom implementations. Ports & Adapters image source: Wikimedia \u21a9","title":"Extensibility"},{"location":"configuration/","text":"Configuration Configuration Files","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#configuration-files","text":"","title":"Configuration Files"},{"location":"configuration/#_1","text":"","title":""},{"location":"deployment/","text":"Deployment TODO","title":"Deployment"},{"location":"deployment/#deployment","text":"TODO","title":"Deployment"},{"location":"jobs/","text":"Jobs Alpakkeer jobs are wrappers around streams with sources which have a definite count of records to process (e.g. JDBC sources, directory listings, etc.). Jobs can be triggered following a schedule, manually via API call or programmatically via its API. Alpakkeer ensures that different executions of a job are not running in parallel. If a job is already running when it is triggered, it can be either queued or discarded. When triggering jobs, properties can be passed to it to customize its behaviour; jobs also have a context to share state between executions of a job. Defining Jobs A job can be defined using a builder API. Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO Properties and Context Every job has Properties type and a Context type. By default these types are set to alpakkeer.core.values.Nothing and akka.Done . Job poperties are the arguments which are passed to a job when it is triggered. The properties can be used while constructing the stream. Every job has default properties which are used if no properties are explicitly passed while triggering. The job's context can be seen as the shared state between job executions. The current context of the job is passed to the stream constructor, when the job is triggered - just like the properties. The result aka the materialzed value of the job execution is the updated context which will be passed to the next execution. To make the context persistent between restarts of the application, Alpakkeer by default offers file-based and database based context persistence. The job types can be defined by passing default properties and the initial context to the job builder API: Java @Value @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Properties { final String name ; } @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Context { Context addGreeted ( String name ) { // ... } } alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" , Properties . apply ( \"Anonymous\" ), Context . apply ()) . runGraph ( b -> { var name = b . getProperties (). getName (); var hello = \"Hello \" + name + \"!\" ; var nextContext = b . getContext (). addGreeted ( name ); return Source . single () . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()) . mapMaterializedValue ( done -> done . thenApply ( i -> nextContext )); }) To enable serialization for the REST API as well as for Alpakkeer's built-in persistence providers, make sure that the class is serializable and de-serializable with Jackson. Triggering Jobs To trigger a job via the REST API with its default properties, call: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/jobs/hello-world To trigger the job with properties, just pass them within the request body: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }' \\ http://localhost:8042/api/v1/jobs/hello-world Jobs can also be triggered programmatically: var app = Alpakkeer . create () /* ... job definitions ... */ . start (); var job = app . getResources (). < Context , Properties > getJob ( \"hello-world\" ); job . start (); job . start ( false ); // do not queue job . start ( false , Properties . apply ( \"Emil\" )); Scheduling Jobs Jobs can be configured to be triggered following a cron expression based schedule. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withScheduledExecution ( CronExpression . apply ( \"0 0 12 * * ?\" ), Properties . apply ( \"Emil\" ), false )) Scala CronExpression offer various helper methods to simplify the creation of the expression, e.g. CronExpression.everySeconds(10) . You may register multiple expressions to schedule different kinds of executions for the job, e.g. an incremental-load every day and a full-load every month for crawler applications. Logging & Monitoring Jobs can be monitored using the interface alpakkeer.core.jobs.monitor.JobMonitor . The monitor can implement methods to collect information about stream execution time, exceptions, results and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface JobMonitor < P , C > { void onTriggered ( String executionId , P properties ); void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause ); void onCompleted ( String executionId , C result ); void onCompleted ( String executionId ); void onStopped ( String executionId , C result ); void onStopped ( String executionId ); void onQueued ( int newQueueSize ); void onEnqueued ( int newQueueSize ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the job status in the job's REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala A job can be configured to have multipe monitors of course. Persisting Context By default all job contexts are kept in-memory only. The context store can be defined in the application configuration: application.conf alpakkeer.contexts.type = \"in-memory\" Other possible values are fs (file-system) or db (database). The context store can also be configured programmatically: Java Alpakkeer . create () . configure ( r -> r . withContextStore ( ContextStores . fileSystem ())) . withJob ( /* ... */ ) /* ... */ . start () Scala Disabling Jobs Jobs can be disabled programmatically. This feature can be used to disable jobs by application configurations. See also Deployment for more information. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isJobEnabled ())) Scala Custom API endpoints The default REST API can be extended with custom REST endpoints, e.g. to trigger the job with custom inputs than job properties. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , job ) -> { api . get ( \"process\" , ctx -> { ctx . result ( job . start (). toCompletableFuture ()); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information. Configuration Overrides Job configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. schedule) in various environments. The default configuration key is alpakkeer.jobs : application.conf alpakkeer.jobs = [ { name = \"hello-world\" enabled = true clear-monitors = false clear-schedule = true monitors = [\"logging\"] schedule = [ { cron-expression = \"0 0 12 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }\"\"\" }, { cron-expression = \"0 0 6 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Antigone\"} }\"\"\" } ] } ] In the job definition, these configurations can be used: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala REST API Jobs can be managed using Alpakkeers REST API. The endpoints for jobs are: GET /api/v1/jobs - List all configured jobs GET /api/v1/jobs/{name} - Get job details POST /api/v1/jobs/{name} - Trigger job execution DELETE /api/v1/jobs/{name} - Stop current job execution GET /api/v1/jobs/{name}/sample - Get example properties which need to be posted when triggering the job For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"Jobs"},{"location":"jobs/#jobs","text":"Alpakkeer jobs are wrappers around streams with sources which have a definite count of records to process (e.g. JDBC sources, directory listings, etc.). Jobs can be triggered following a schedule, manually via API call or programmatically via its API. Alpakkeer ensures that different executions of a job are not running in parallel. If a job is already running when it is triggered, it can be either queued or discarded. When triggering jobs, properties can be passed to it to customize its behaviour; jobs also have a context to share state between executions of a job.","title":"Jobs"},{"location":"jobs/#defining-jobs","text":"A job can be defined using a builder API. Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO","title":"Defining Jobs"},{"location":"jobs/#properties-and-context","text":"Every job has Properties type and a Context type. By default these types are set to alpakkeer.core.values.Nothing and akka.Done . Job poperties are the arguments which are passed to a job when it is triggered. The properties can be used while constructing the stream. Every job has default properties which are used if no properties are explicitly passed while triggering. The job's context can be seen as the shared state between job executions. The current context of the job is passed to the stream constructor, when the job is triggered - just like the properties. The result aka the materialzed value of the job execution is the updated context which will be passed to the next execution. To make the context persistent between restarts of the application, Alpakkeer by default offers file-based and database based context persistence. The job types can be defined by passing default properties and the initial context to the job builder API: Java @Value @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Properties { final String name ; } @NoArgsConstructor @AllArgsConstructor ( staticName = \"apply\" ) public class Context { Context addGreeted ( String name ) { // ... } } alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" , Properties . apply ( \"Anonymous\" ), Context . apply ()) . runGraph ( b -> { var name = b . getProperties (). getName (); var hello = \"Hello \" + name + \"!\" ; var nextContext = b . getContext (). addGreeted ( name ); return Source . single () . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()) . mapMaterializedValue ( done -> done . thenApply ( i -> nextContext )); }) To enable serialization for the REST API as well as for Alpakkeer's built-in persistence providers, make sure that the class is serializable and de-serializable with Jackson.","title":"Properties and Context"},{"location":"jobs/#triggering-jobs","text":"To trigger a job via the REST API with its default properties, call: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/jobs/hello-world To trigger the job with properties, just pass them within the request body: curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }' \\ http://localhost:8042/api/v1/jobs/hello-world Jobs can also be triggered programmatically: var app = Alpakkeer . create () /* ... job definitions ... */ . start (); var job = app . getResources (). < Context , Properties > getJob ( \"hello-world\" ); job . start (); job . start ( false ); // do not queue job . start ( false , Properties . apply ( \"Emil\" ));","title":"Triggering Jobs"},{"location":"jobs/#scheduling-jobs","text":"Jobs can be configured to be triggered following a cron expression based schedule. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withScheduledExecution ( CronExpression . apply ( \"0 0 12 * * ?\" ), Properties . apply ( \"Emil\" ), false )) Scala CronExpression offer various helper methods to simplify the creation of the expression, e.g. CronExpression.everySeconds(10) . You may register multiple expressions to schedule different kinds of executions for the job, e.g. an incremental-load every day and a full-load every month for crawler applications.","title":"Scheduling Jobs"},{"location":"jobs/#logging-monitoring","text":"Jobs can be monitored using the interface alpakkeer.core.jobs.monitor.JobMonitor . The monitor can implement methods to collect information about stream execution time, exceptions, results and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface JobMonitor < P , C > { void onTriggered ( String executionId , P properties ); void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause ); void onCompleted ( String executionId , C result ); void onCompleted ( String executionId ); void onStopped ( String executionId , C result ); void onStopped ( String executionId ); void onQueued ( int newQueueSize ); void onEnqueued ( int newQueueSize ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the job status in the job's REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala A job can be configured to have multipe monitors of course.","title":"Logging &amp; Monitoring"},{"location":"jobs/#persisting-context","text":"By default all job contexts are kept in-memory only. The context store can be defined in the application configuration: application.conf alpakkeer.contexts.type = \"in-memory\" Other possible values are fs (file-system) or db (database). The context store can also be configured programmatically: Java Alpakkeer . create () . configure ( r -> r . withContextStore ( ContextStores . fileSystem ())) . withJob ( /* ... */ ) /* ... */ . start () Scala","title":"Persisting Context"},{"location":"jobs/#disabling-jobs","text":"Jobs can be disabled programmatically. This feature can be used to disable jobs by application configurations. See also Deployment for more information. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isJobEnabled ())) Scala","title":"Disabling Jobs"},{"location":"jobs/#custom-api-endpoints","text":"The default REST API can be extended with custom REST endpoints, e.g. to trigger the job with custom inputs than job properties. Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , job ) -> { api . get ( \"process\" , ctx -> { ctx . result ( job . start (). toCompletableFuture ()); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information.","title":"Custom API endpoints"},{"location":"jobs/#configuration-overrides","text":"Job configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. schedule) in various environments. The default configuration key is alpakkeer.jobs : application.conf alpakkeer.jobs = [ { name = \"hello-world\" enabled = true clear-monitors = false clear-schedule = true monitors = [\"logging\"] schedule = [ { cron-expression = \"0 0 12 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Edgar\"} }\"\"\" }, { cron-expression = \"0 0 6 * * ?\" queue = false properties = \"\"\"{\"queue\": true, \"properties\": { \"name\": \"Antigone\"} }\"\"\" } ] } ] In the job definition, these configurations can be used: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala","title":"Configuration Overrides"},{"location":"jobs/#rest-api","text":"Jobs can be managed using Alpakkeers REST API. The endpoints for jobs are: GET /api/v1/jobs - List all configured jobs GET /api/v1/jobs/{name} - Get job details POST /api/v1/jobs/{name} - Trigger job execution DELETE /api/v1/jobs/{name} - Stop current job execution GET /api/v1/jobs/{name}/sample - Get example properties which need to be posted when triggering the job For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"REST API"},{"location":"monitoring/","text":"Monitoring & Metrics As described for Jobs and Processes , Alpakkeer offers monitors and building blocks for argumenting streams with monitoring capabilities. Prometheus Integration Grafana Integration","title":"Monitoring & Metrics"},{"location":"monitoring/#monitoring-metrics","text":"As described for Jobs and Processes , Alpakkeer offers monitors and building blocks for argumenting streams with monitoring capabilities.","title":"Monitoring &amp; Metrics"},{"location":"monitoring/#prometheus-integration","text":"","title":"Prometheus Integration"},{"location":"monitoring/#grafana-integration","text":"","title":"Grafana Integration"},{"location":"processes/","text":"Processes Processes are wrappers around streams with sources which have an indefinte count of records to process (e.g. Kafka consumers, directory change listeners, ...). Processes are usually running continously as long as the application is up and running, but they can also be stopped and restarted manually. If the stream fails, Alpakkeer will automatically try to restart the stream with a restart-with-backoff mechanism. Defining Processes A process can be defined using a builder API. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> Source . repeat ( \"Hello World\" ) . throttle ( 1 , Duration . ofSeconds ( 1 )) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala // TODO Starting and Stopping Processes can be started and stopped via REST API. Start curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/processes/hello-world Stop curl \\ --header \"Content-Type: application/json\" \\ --request DELETE \\ http://localhost:8042/api/v1/processes/hello-world Starting and stopping is also possible programmatically: Java var app = Alpakkeer . create () /* ... process definitions ... */ . start (); var process = app . getResources (). getProcess ( \"hello-world\" ); process . stop (); process . start (); Scala Processes are by default started upon application startup, this behavior can be disabled: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . initializeStopped ()) Scala Configuring Restart Backoffs Alpakkeer will try to keep streams running as long as the process is not stopped. Thus Alpakkeer restarts gracefully repeated streams, when a stream fails with an exception it is also restarted, but with an increasing backoff-timeout. The restart timeouts can be defined: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withCompletionRestartBackoff ( Duration . ofMinutes ( 5 )) . withInitialRetryBackoff ( Duration . ofSeconds ( 30 )) . withRetryBackoffResetTimeout ( Duration . ofMinutes ( 10 ))) Scala The retry-backoff-reset-timeout specifies the duration how long a stream must be running without failure to reset the retry-backoff to its initial value. Logging & Monitoring Processes can be monitored using the interface alpakkeer.core.processes.monitor.ProcessMonitor . The monitor can implement methods to collect information about exceptions, completions, start- and stop-events and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface ProcessMonitor { void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause , Instant nextRetry ); void onCompletion ( String executionId , Instant nextStart ); void onStopped ( String executionId ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the stream status in the processes' REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala Aa process can be configured to have multipe monitors of course. Disabling Processes Processes can be disabled programmatically. This feature can be used to disable processes by application configurations. See also Deployment for more information. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isProcessEnabled ())) Scala API endpoints The default REST API can be extended with custom REST endpoints. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , process ) -> { api . get ( \"process\" , ctx -> { process . stop () ctx . result ( \"stopped\" ); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information. Configuration Overrides Process configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. different backoff-timeouts) in various environments. The default configuration key is alpakkeer.processes : TODO Backoff Configuration application.conf alpakkeer.processes = [ { name = \"hello-world\" enabled = true clear-monitors = false monitors = [\"logging\"] initialize-started = false } ] In the process definition, these configurations can be used: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala REST API Processes can be managed using Alpakkeers REST API. The endpoints for processes are: GET /api/v1/processes - List all defined processes GET /api/v1/processes/{name} - Get process details POST /api/v1/processes/{name} - Start the process DELETE /api/v1/processes/{name} - Stop the process For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"Processes"},{"location":"processes/#processes","text":"Processes are wrappers around streams with sources which have an indefinte count of records to process (e.g. Kafka consumers, directory change listeners, ...). Processes are usually running continously as long as the application is up and running, but they can also be stopped and restarted manually. If the stream fails, Alpakkeer will automatically try to restart the stream with a restart-with-backoff mechanism.","title":"Processes"},{"location":"processes/#defining-processes","text":"A process can be defined using a builder API. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> Source . repeat ( \"Hello World\" ) . throttle ( 1 , Duration . ofSeconds ( 1 )) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala // TODO","title":"Defining Processes"},{"location":"processes/#starting-and-stopping","text":"Processes can be started and stopped via REST API. Start curl \\ --header \"Content-Type: application/json\" \\ --request POST \\ http://localhost:8042/api/v1/processes/hello-world Stop curl \\ --header \"Content-Type: application/json\" \\ --request DELETE \\ http://localhost:8042/api/v1/processes/hello-world Starting and stopping is also possible programmatically: Java var app = Alpakkeer . create () /* ... process definitions ... */ . start (); var process = app . getResources (). getProcess ( \"hello-world\" ); process . stop (); process . start (); Scala Processes are by default started upon application startup, this behavior can be disabled: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . initializeStopped ()) Scala","title":"Starting and Stopping"},{"location":"processes/#configuring-restart-backoffs","text":"Alpakkeer will try to keep streams running as long as the process is not stopped. Thus Alpakkeer restarts gracefully repeated streams, when a stream fails with an exception it is also restarted, but with an increasing backoff-timeout. The restart timeouts can be defined: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withCompletionRestartBackoff ( Duration . ofMinutes ( 5 )) . withInitialRetryBackoff ( Duration . ofSeconds ( 30 )) . withRetryBackoffResetTimeout ( Duration . ofMinutes ( 10 ))) Scala The retry-backoff-reset-timeout specifies the duration how long a stream must be running without failure to reset the retry-backoff to its initial value.","title":"Configuring Restart Backoffs"},{"location":"processes/#logging-monitoring","text":"Processes can be monitored using the interface alpakkeer.core.processes.monitor.ProcessMonitor . The monitor can implement methods to collect information about exceptions, completions, start- and stop-events and other stream metrics collected with monitoring flows from Alpakkeer stream utilities. Java public interface ProcessMonitor { void onStarted ( String executionId ); void onFailed ( String executionId , Throwable cause , Instant nextRetry ); void onCompletion ( String executionId , Instant nextStart ); void onStopped ( String executionId ); CompletionStage < Optional < Object >> getStatus (); void onStats ( String executionId , String name , CheckpointMonitor . Stats statistics ); void onStats ( String executionId , String name , LatencyMonitor . Stats statistics ); } The method getStatus can return an arbitary object which will be part of the stream status in the processes' REST API. Alpakkeer comes with a few built-in monitors: The LoggingMonitor will log all information into the SLF4J logger. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withLoggingMonitor ()) Scala The PrometheusMonitor will feed all information into Alpakkeer's Prometheus collector registry. These metrics can then be accessed by Prometheus via /metrics . Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withPrometheusMonitor ()) Scala The HistoryMonitor stores information of executions up-to a define age or count of executions. The information can be kept in memory or can be stored in a database. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withHistoryMonitor ()) Scala Aa process can be configured to have multipe monitors of course.","title":"Logging &amp; Monitoring"},{"location":"processes/#disabling-processes","text":"Processes can be disabled programmatically. This feature can be used to disable processes by application configurations. See also Deployment for more information. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . disable ( config . isProcessEnabled ())) Scala","title":"Disabling Processes"},{"location":"processes/#api-endpoints","text":"The default REST API can be extended with custom REST endpoints. Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( api , process ) -> { api . get ( \"process\" , ctx -> { process . stop () ctx . result ( \"stopped\" ); }); })) Scala The api object is an instance of Javalin , you can also extend the endpint with Open API documentation. See Javalin Docs for detailed information.","title":"API endpoints"},{"location":"processes/#configuration-overrides","text":"Process configurations can be overridden or extended by (environment-specific) configurations. This feature might be useful if the stream should run with different configurations (e.g. different backoff-timeouts) in various environments. The default configuration key is alpakkeer.processes : TODO Backoff Configuration application.conf alpakkeer.processes = [ { name = \"hello-world\" enabled = true clear-monitors = false monitors = [\"logging\"] initialize-started = false } ] In the process definition, these configurations can be used: Java alpakkeer . withProcess ( processes -> processes . create ( /* ... */ ) . runGraph ( /* ... */ ) . withConfiguration ()) Scala","title":"Configuration Overrides"},{"location":"processes/#rest-api","text":"Processes can be managed using Alpakkeers REST API. The endpoints for processes are: GET /api/v1/processes - List all defined processes GET /api/v1/processes/{name} - Get process details POST /api/v1/processes/{name} - Start the process DELETE /api/v1/processes/{name} - Stop the process For more details, visit the Open API spec on a running instance, e.g. http://localhost:8042 .","title":"REST API"},{"location":"stream-utilities/","text":"Stream Utilities Alpakkeer offers easy accessible building-blocks for composing reactive streams with Akka Streams. These utilities can be accessed via the builder object which is passed to runGraph -methods of the Job- and Process-Builders. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( streamBuilder -> streamBuilder . messaging () . recordsSource ( \"topic\" , String . class ) . throttle ( 1 , Duration . ofSeconds ( 1 )) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala // TODO Messaging The messaging building-blocks are an abstraction over messaging technologies, such as Kafka, Google PubSub or JMS. Alpakkeer also offers simple built-in messaging in-memory and file-system implementations which can be used for local development or simple systems. The abstraction allows developers to develop, run and test a stream-topology fast and quickly on the local machine w/o involving other technologies like Docker etc. for running Kafka, JMS brokers, etc.. Also automated component tests can be written quite easily using in-memory implementations. The messaging implementations take also care about re-processing failed (not committed) messages and committing offsets or elements (depending on the technology). Supported Messaging Patterns Alpakkeer supports the message-queue and publish-subscribe messaging pattern. While message-queue has a queue where each message is processed at-least-once from any consumer, the publish-subscribe pattern has a topic where each message is processed at-least-once by every listening consumer. Details like message retention etc. are specific to messaging providers. By default messages which is sent via the messaging layer is serialized and deserialized using Alpakkeer's internal ObjectMapper instance, thus all messages will be serialized and sent as JSON messages. While this is not the most performant way of serializing and deserializing, it's good enough for most real-world usecases. Nevertheless, custom messaging providers may also user other serializers and deserializers. Every message must have a unique-key. If there is no meaningful key available in the used data model, Alpakkeer creates a unique key automatically. Producing Messages A sink to a messaging queue or topic can be created as follows: Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( b . messaging (). itemsSink ( \"some_topic\" , s -> s . toLowerCase ()), Keep . right ())) Scala // TODO The 2nd parameter of itemsSink is a function which maps an item to its id. If the function is not defined, Aalpakkeer will create a randomized unique id. Single messages can also be produced without a stream, e.g. when a message should be created upon a Web API call: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( app , api , job ) -> { api . get ( \"process\" , ctx -> { var request = ctx . bodyAsClass ( Request . class ); var result = app . getMessaging () . putItem ( \"input\" , request ) . thenApply ( done -> \"Ok\" ) ctx . result ( result . toCompletableFuture ()); }); })) Scala Consuming Messages Messages can be consumed by using Source -methods from Alpakkeer's messaging interface. Optionally a consumer group name can be passed to each of the sources to identify multiple consumers. If the consumer group name is ommitted, a default will be used. At runtime there might be multipe instances using the same consumer group name, the whole group will process each message at least once. The output of messaging sources is always a Record . A record is an envelope around the actual message type which carries context information and the actual message. Each record should be committed, when it is successfully processed. All Alpakkeer sinks will automatically commit processed messages, but if no built-in sink is used, the record must be committed manually. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> b . getMessaging () . recordsSource ( \"process-input\" , classOf [ Document ] , \"processors\" ) . map ( record -> record . withValue ( record . getValue (). getName ())) . toMat ( b . messaging (). recordsSink ( \"some_topic\" ), Keep . right ()))) Scala Consuming and Producing Messages When a stream consumes, processes messaages and writes them to another topic or queue, it should use a record sink. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> b . getMessaging () . recordsSource ( \"process-input\" , classOf [ Document ] , \"processors\" ) . map ( record -> { record . commit (); return record . getValue (); }) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala Configuration The messaging type can be specified in Alpakkeer's messaaging configuration: application.conf alpakkeer.messaging { # Possible values: in-memory, fs, kafka type = \"in-memory\" fs { directory = \"./messaging\" } kafka { bootstrap-server: \"localhost:9042\" consumer { # See Alpakka Kafka Connector consumer configurations } producer { # See Alpakka Kafka Connector producer configurations } } } TODO: Topic and Queue configurations. Stream Monitoring Alpakkeer offers sub-flows to argument streams with additional metrics for monitoring. Checkpoints collect simple measures at any point within a stream and report them in a defined interval to the proccess- or job-monitors: Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . via ( b . getMonitoring (). createCheckpointMonitor ( \"step_1\" , Duration . ofSeconds ( 10 ))) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO The checkpoint will report the following metrics: moment : The moment when the checkpoint published its statistic as Java Instant timeElapsedNanos : The time elapsed in the interval in nano seconds count : The number of documents processed in the interval totalPushPullLatencyNanos : The total latency of the checkpoint's graph stage between push and next pull-call from the downstream totalPullPushLatencyNanos : The total latency of the checkpoint's graph stage between pull and push of the upstrean Thus the checkpoint metrics can detect whether there are problems in the down- or the upstream processing, as well as simply count the number of processed documents in a stream. To monitor processing time of a sub-flow a Latency Monitor can be used. The sub-flow which is monitored must not filter or loose any elements. Java var flow = Flow . < String > create (). map ( String :: toUpperCase ); alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . via ( b . getMonitoring (). createLatencyMonitor ( \"sub-flow\" , flow , Duration . ofSeconds ( 10 ))) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO Like the checkpoint monitor, the latency monitor emits statistics in a given interval. The statistics reported include: moment : The moment when the monitor published its statistic as Java Instant timeElapsedNanos : The time elapsed in the interval in nano seconds count : The number of documents processed in the interval sumLatency : The complete processing time within the interval in nano seconds avgLatency : The average processing time measured in the interval All statistics are reported to the configured job- or process-monitors. Depending on the monitor implemention the statistics are logged, summerized or collected (e.g. for Prometheus). Error Handling TODO","title":"Stream Utilities"},{"location":"stream-utilities/#stream-utilities","text":"Alpakkeer offers easy accessible building-blocks for composing reactive streams with Akka Streams. These utilities can be accessed via the builder object which is passed to runGraph -methods of the Job- and Process-Builders. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( streamBuilder -> streamBuilder . messaging () . recordsSource ( \"topic\" , String . class ) . throttle ( 1 , Duration . ofSeconds ( 1 )) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala // TODO","title":"Stream Utilities"},{"location":"stream-utilities/#messaging","text":"The messaging building-blocks are an abstraction over messaging technologies, such as Kafka, Google PubSub or JMS. Alpakkeer also offers simple built-in messaging in-memory and file-system implementations which can be used for local development or simple systems. The abstraction allows developers to develop, run and test a stream-topology fast and quickly on the local machine w/o involving other technologies like Docker etc. for running Kafka, JMS brokers, etc.. Also automated component tests can be written quite easily using in-memory implementations. The messaging implementations take also care about re-processing failed (not committed) messages and committing offsets or elements (depending on the technology).","title":"Messaging"},{"location":"stream-utilities/#supported-messaging-patterns","text":"Alpakkeer supports the message-queue and publish-subscribe messaging pattern. While message-queue has a queue where each message is processed at-least-once from any consumer, the publish-subscribe pattern has a topic where each message is processed at-least-once by every listening consumer. Details like message retention etc. are specific to messaging providers. By default messages which is sent via the messaging layer is serialized and deserialized using Alpakkeer's internal ObjectMapper instance, thus all messages will be serialized and sent as JSON messages. While this is not the most performant way of serializing and deserializing, it's good enough for most real-world usecases. Nevertheless, custom messaging providers may also user other serializers and deserializers. Every message must have a unique-key. If there is no meaningful key available in the used data model, Alpakkeer creates a unique key automatically.","title":"Supported Messaging Patterns"},{"location":"stream-utilities/#producing-messages","text":"A sink to a messaging queue or topic can be created as follows: Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . toMat ( b . messaging (). itemsSink ( \"some_topic\" , s -> s . toLowerCase ()), Keep . right ())) Scala // TODO The 2nd parameter of itemsSink is a function which maps an item to its id. If the function is not defined, Aalpakkeer will create a randomized unique id. Single messages can also be produced without a stream, e.g. when a message should be created upon a Web API call: Java alpakkeer . withJob ( jobs -> jobs . create ( /* ... */ ) . runGraph ( /* ... */ ) . withApiEndpoint (( app , api , job ) -> { api . get ( \"process\" , ctx -> { var request = ctx . bodyAsClass ( Request . class ); var result = app . getMessaging () . putItem ( \"input\" , request ) . thenApply ( done -> \"Ok\" ) ctx . result ( result . toCompletableFuture ()); }); })) Scala","title":"Producing Messages"},{"location":"stream-utilities/#consuming-messages","text":"Messages can be consumed by using Source -methods from Alpakkeer's messaging interface. Optionally a consumer group name can be passed to each of the sources to identify multiple consumers. If the consumer group name is ommitted, a default will be used. At runtime there might be multipe instances using the same consumer group name, the whole group will process each message at least once. The output of messaging sources is always a Record . A record is an envelope around the actual message type which carries context information and the actual message. Each record should be committed, when it is successfully processed. All Alpakkeer sinks will automatically commit processed messages, but if no built-in sink is used, the record must be committed manually. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> b . getMessaging () . recordsSource ( \"process-input\" , classOf [ Document ] , \"processors\" ) . map ( record -> record . withValue ( record . getValue (). getName ())) . toMat ( b . messaging (). recordsSink ( \"some_topic\" ), Keep . right ()))) Scala","title":"Consuming Messages"},{"location":"stream-utilities/#consuming-and-producing-messages","text":"When a stream consumes, processes messaages and writes them to another topic or queue, it should use a record sink. Java alpakkeer . withProcess ( process -> process . create ( \"hello-world\" ) . runGraph ( b -> b . getMessaging () . recordsSource ( \"process-input\" , classOf [ Document ] , \"processors\" ) . map ( record -> { record . commit (); return record . getValue (); }) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ()))) Scala","title":"Consuming and Producing Messages"},{"location":"stream-utilities/#configuration","text":"The messaging type can be specified in Alpakkeer's messaaging configuration: application.conf alpakkeer.messaging { # Possible values: in-memory, fs, kafka type = \"in-memory\" fs { directory = \"./messaging\" } kafka { bootstrap-server: \"localhost:9042\" consumer { # See Alpakka Kafka Connector consumer configurations } producer { # See Alpakka Kafka Connector producer configurations } } } TODO: Topic and Queue configurations.","title":"Configuration"},{"location":"stream-utilities/#stream-monitoring","text":"Alpakkeer offers sub-flows to argument streams with additional metrics for monitoring. Checkpoints collect simple measures at any point within a stream and report them in a defined interval to the proccess- or job-monitors: Java alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . via ( b . getMonitoring (). createCheckpointMonitor ( \"step_1\" , Duration . ofSeconds ( 10 ))) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO The checkpoint will report the following metrics: moment : The moment when the checkpoint published its statistic as Java Instant timeElapsedNanos : The time elapsed in the interval in nano seconds count : The number of documents processed in the interval totalPushPullLatencyNanos : The total latency of the checkpoint's graph stage between push and next pull-call from the downstream totalPullPushLatencyNanos : The total latency of the checkpoint's graph stage between pull and push of the upstrean Thus the checkpoint metrics can detect whether there are problems in the down- or the upstream processing, as well as simply count the number of processed documents in a stream. To monitor processing time of a sub-flow a Latency Monitor can be used. The sub-flow which is monitored must not filter or loose any elements. Java var flow = Flow . < String > create (). map ( String :: toUpperCase ); alpakkeer . withJob ( jobs -> jobs . create ( \"hello-world\" ) . runGraph ( b -> Source . single ( \"Hello World\" ) . via ( b . getMonitoring (). createLatencyMonitor ( \"sub-flow\" , flow , Duration . ofSeconds ( 10 ))) . toMat ( Sink . foreach ( System . out :: println ), Keep . right ())) Scala // TODO Like the checkpoint monitor, the latency monitor emits statistics in a given interval. The statistics reported include: moment : The moment when the monitor published its statistic as Java Instant timeElapsedNanos : The time elapsed in the interval in nano seconds count : The number of documents processed in the interval sumLatency : The complete processing time within the interval in nano seconds avgLatency : The average processing time measured in the interval All statistics are reported to the configured job- or process-monitors. Depending on the monitor implemention the statistics are logged, summerized or collected (e.g. for Prometheus).","title":"Stream Monitoring"},{"location":"stream-utilities/#error-handling","text":"TODO","title":"Error Handling"}]}